{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIMO3 Training on OpenMathReasoning Dataset\n",
    "\n",
    "Fine-tune a math-capable model on the OpenMathReasoning dataset for AIMO3 competition.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repo\n",
    "!git clone https://github.com/kkhskh/1.git\n",
    "%cd 1\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q datasets transformers peft accelerate torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with Hugging Face (for model access)\n",
    "from huggingface_hub import login\n",
    "\n",
    "# You'll need to get a token from https://huggingface.co/settings/tokens\n",
    "# Make sure to accept the terms for any gated models\n",
    "login()  # This will prompt for your token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED: Use smaller model + 4-bit quantization for Colab T4\n",
    "# model_name = \"Qwen/Qwen2.5-Math-7B\"  # Too big for T4\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"  # SMALLER: ~3GB VRAM with 4-bit\n",
    "# model_name = \"microsoft/Phi-3-mini-4k-instruct\"  # Alternative: ~2GB VRAM\n",
    "\n",
    "# Training configuration - FULL DATASET with Colab Pro\n",
    "num_train_epochs = 3\n",
    "max_samples = 0  # 0 = use FULL dataset (~1.8M samples)\n",
    "batch_size = 4  # Higher batch size for A100/V100\n",
    "learning_rate = 1e-5  # Lower learning rate for full training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training\n",
    "import os\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['MODEL_NAME'] = model_name\n",
    "os.environ['NUM_EPOCHS'] = str(num_train_epochs)\n",
    "os.environ['MAX_SAMPLES'] = str(max_samples)\n",
    "os.environ['MAX_COT'] = str(0)\n",
    "os.environ['MAX_TIR'] = str(0)\n",
    "os.environ['MAX_STEPS'] = str(600)\n",
    "os.environ['BATCH_SIZE'] = str(batch_size)\n",
    "os.environ['LEARNING_RATE'] = str(learning_rate)\n",
    "\n",
    "# Run training\n",
    "!python train_openmath.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After Training\n",
    "\n",
    "1. Download the trained model from `./models/openmath-finetuned/`\n",
    "2. Test it locally or use it in your competition pipeline\n",
    "3. For inference, you can use the model with your existing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model (after training)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load the trained model\n",
    "model_path = \"./models/openmath-finetuned\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Test problem\n",
    "test_problem = \"Solve for x: 2x + 3 = 7\"\n",
    "prompt = f\"Problem: {test_problem}\\n\\nSolution: \"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.1,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Model response:\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
